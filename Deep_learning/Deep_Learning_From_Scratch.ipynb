{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "# Deep Learning From Scratch: A Self-Learning Journey\n",
    "\n",
    "Welcome to your comprehensive deep learning tutorial! This notebook will guide you through building neural networks from scratch using only Python and basic mathematical libraries.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the fundamental concepts of neural networks\n",
    "- Implement a neural network from scratch without using deep learning frameworks\n",
    "- Master forward propagation and backpropagation algorithms\n",
    "- Apply your knowledge to classify the famous Iris dataset\n",
    "- Evaluate and optimize your neural network's performance\n",
    "\n",
    "## üõ†Ô∏è Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Understanding of linear algebra (vectors, matrices, dot products)\n",
    "- Basic calculus (derivatives, chain rule)\n",
    "- Familiarity with NumPy\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. **Introduction to Neural Networks**\n",
    "2. **Mathematical Foundations**\n",
    "3. **Data Preparation: The Iris Dataset**\n",
    "4. **Building Our Neural Network Architecture**\n",
    "5. **Forward Propagation**\n",
    "6. **Backpropagation and Gradient Descent**\n",
    "7. **Training the Neural Network**\n",
    "8. **Model Evaluation and Visualization**\n",
    "9. **Exercises and Challenges**\n",
    "10. **Next Steps and Advanced Topics**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Getting Started\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "NumPy version: 2.3.2\n",
      "Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-intro",
   "metadata": {},
   "source": [
    "# 1. Introduction to Neural Networks üß†\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "A neural network is a computational model inspired by the way biological neural networks in animal brains process information. It consists of:\n",
    "\n",
    "- **Neurons (Nodes)**: Processing units that receive inputs, perform calculations, and produce outputs\n",
    "- **Weights**: Parameters that determine the strength of connections between neurons\n",
    "- **Biases**: Additional parameters that allow neurons to learn patterns more effectively\n",
    "- **Activation Functions**: Functions that introduce non-linearity to the network\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "1. **Input Layer**: Receives the input data\n",
    "2. **Hidden Layer(s)**: Processes the data through weighted connections\n",
    "3. **Output Layer**: Produces the final predictions\n",
    "\n",
    "## Why Build From Scratch?\n",
    "\n",
    "Building neural networks from scratch helps you:\n",
    "- Understand the underlying mathematics\n",
    "- Debug and optimize models more effectively\n",
    "- Implement custom architectures\n",
    "- Gain deep intuition about how neural networks work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-foundations",
   "metadata": {},
   "source": [
    "# 2. Mathematical Foundations üìê\n",
    "\n",
    "## Essential Mathematical Concepts\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity to neural networks, allowing them to learn complex patterns.\n",
    "\n",
    "Let's implement some common activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        \"\"\"ReLU activation function: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        \"\"\"Derivative of tanh function\"\"\"\n",
    "        return 1 - x**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        \"\"\"Softmax activation function for multi-class classification\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Let's visualize these activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Common Activation Functions', fontsize=16)\n",
    "\n",
    "# Sigmoid\n",
    "axes[0,0].plot(x, ActivationFunctions.sigmoid(x), 'b-', linewidth=2)\n",
    "axes[0,0].set_title('Sigmoid Function')\n",
    "axes[0,0].set_xlabel('x')\n",
    "axes[0,0].set_ylabel('f(x)')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# ReLU\n",
    "axes[0,1].plot(x, ActivationFunctions.relu(x), 'r-', linewidth=2)\n",
    "axes[0,1].set_title('ReLU Function')\n",
    "axes[0,1].set_xlabel('x')\n",
    "axes[0,1].set_ylabel('f(x)')\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Tanh\n",
    "axes[1,0].plot(x, ActivationFunctions.tanh(x), 'g-', linewidth=2)\n",
    "axes[1,0].set_title('Tanh Function')\n",
    "axes[1,0].set_xlabel('x')\n",
    "axes[1,0].set_ylabel('f(x)')\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# Comparison\n",
    "axes[1,1].plot(x, ActivationFunctions.sigmoid(x), 'b-', label='Sigmoid', linewidth=2)\n",
    "axes[1,1].plot(x, ActivationFunctions.relu(x), 'r-', label='ReLU', linewidth=2)\n",
    "axes[1,1].plot(x, ActivationFunctions.tanh(x), 'g-', label='Tanh', linewidth=2)\n",
    "axes[1,1].set_title('Function Comparison')\n",
    "axes[1,1].set_xlabel('x')\n",
    "axes[1,1].set_ylabel('f(x)')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Activation functions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iris-dataset",
   "metadata": {},
   "source": [
    "# 3. Data Preparation: The Iris Dataset üå∏\n",
    "\n",
    "The Iris dataset is a classic dataset in machine learning, perfect for learning classification. It contains measurements of iris flowers from three different species.\n",
    "\n",
    "## Dataset Features:\n",
    "- **Sepal Length** (cm)\n",
    "- **Sepal Width** (cm) \n",
    "- **Petal Length** (cm)\n",
    "- **Petal Width** (cm)\n",
    "\n",
    "## Target Classes:\n",
    "- **Setosa** (0)\n",
    "- **Versicolor** (1)\n",
    "- **Virginica** (2)\n",
    "\n",
    "Let's load and explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[y]\n",
    "df['target'] = y\n",
    "\n",
    "print(\"üå∏ Iris Dataset Overview:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {list(iris.feature_names)}\")\n",
    "print(f\"Classes: {list(iris.target_names)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Iris Dataset Exploration', fontsize=16)\n",
    "\n",
    "# Pairplot-style visualization\n",
    "features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "colors = ['red', 'blue', 'green']\n",
    "species = iris.target_names\n",
    "\n",
    "# Scatter plots\n",
    "axes[0,0].scatter(df[features[0]], df[features[1]], c=[colors[i] for i in df['target']], alpha=0.7)\n",
    "axes[0,0].set_xlabel(features[0])\n",
    "axes[0,0].set_ylabel(features[1])\n",
    "axes[0,0].set_title('Sepal Length vs Sepal Width')\n",
    "\n",
    "axes[0,1].scatter(df[features[2]], df[features[3]], c=[colors[i] for i in df['target']], alpha=0.7)\n",
    "axes[0,1].set_xlabel(features[2])\n",
    "axes[0,1].set_ylabel(features[3])\n",
    "axes[0,1].set_title('Petal Length vs Petal Width')\n",
    "\n",
    "# Box plots\n",
    "df.boxplot(column=features[0], by='species', ax=axes[1,0])\n",
    "axes[1,0].set_title('Sepal Length by Species')\n",
    "axes[1,0].set_xlabel('Species')\n",
    "\n",
    "df.boxplot(column=features[2], by='species', ax=axes[1,1])\n",
    "axes[1,1].set_title('Petal Length by Species')\n",
    "axes[1,1].set_xlabel('Species')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], \n",
    "                             markersize=10, label=species[i]) for i in range(3)]\n",
    "fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preprocessing",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before training our neural network, we need to:\n",
    "1. Split the data into training and testing sets\n",
    "2. Standardize the features (important for neural networks)\n",
    "3. Convert labels to one-hot encoding for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"Convert integer labels to one-hot encoding\"\"\"\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = to_one_hot(y_train, 3)\n",
    "y_test_onehot = to_one_hot(y_test, 3)\n",
    "\n",
    "print(\"üîÑ Data preprocessing completed!\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Training labels shape: {y_train_onehot.shape}\")\n",
    "print(f\"Test labels shape: {y_test_onehot.shape}\")\n",
    "\n",
    "# Show example of one-hot encoding\n",
    "print(\"\\nExample of one-hot encoding:\")\n",
    "print(f\"Original labels: {y_train[:5]}\")\n",
    "print(f\"One-hot encoded:\\n{y_train_onehot[:5]}\")\n",
    "\n",
    "# Verify class distribution in train/test splits\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for i, (cls, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"{iris.target_names[cls]}: {count} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for i, (cls, count) in enumerate(zip(unique, counts)):\n",
    "    print(f\"{iris.target_names[cls]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network-class",
   "metadata": {},
   "source": [
    "# 4. Building Our Neural Network Architecture üèóÔ∏è\n",
    "\n",
    "Now let's build our neural network from scratch! We'll create a flexible class that can handle different architectures.\n",
    "\n",
    "## Network Architecture:\n",
    "- **Input Layer**: 4 neurons (for 4 features)\n",
    "- **Hidden Layer**: 8 neurons (with sigmoid activation)\n",
    "- **Output Layer**: 3 neurons (for 3 classes, with softmax activation)\n",
    "\n",
    "## Key Methods:\n",
    "1. `forward_propagation()`: Calculate predictions\n",
    "2. `backward_propagation()`: Calculate gradients\n",
    "3. `update_weights()`: Update network parameters\n",
    "4. `train()`: Train the network\n",
    "5. `predict()`: Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-network-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"A simple neural network built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output classes\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        # Store training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization\"\"\"\n",
    "        # Weights from input to hidden layer\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0 / self.input_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        # Weights from hidden to output layer\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0 / self.hidden_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            \n",
    "        Returns:\n",
    "            A2: Output predictions\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Linear transformation\n",
    "        self.A1 = ActivationFunctions.sigmoid(self.Z1)  # Activation\n",
    "        \n",
    "        # Output layer\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Linear transformation\n",
    "        self.A2 = ActivationFunctions.softmax(self.Z2)  # Softmax for multi-class\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute categorical cross-entropy loss\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            y_pred: Predicted probabilities\n",
    "            \n",
    "        Returns:\n",
    "            loss: Cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        \n",
    "        # Prevent log(0) by adding small epsilon\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Perform backward propagation (backpropagation)\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            y_true: True labels (one-hot encoded)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Calculate gradients for output layer\n",
    "        dZ2 = self.A2 - y_true  # Gradient of loss w.r.t Z2\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m  # Gradient w.r.t W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient w.r.t b2\n",
    "        \n",
    "        # Calculate gradients for hidden layer\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # Gradient w.r.t A1\n",
    "        dZ1 = dA1 * ActivationFunctions.sigmoid_derivative(self.A1)  # Gradient w.r.t Z1\n",
    "        dW1 = np.dot(X.T, dZ1) / m  # Gradient w.r.t W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient w.r.t b1\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dW1, self.db1 = dW1, db1\n",
    "        self.dW2, self.db2 = dW2, db2\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update weights and biases using gradient descent\"\"\"\n",
    "        self.W1 -= self.learning_rate * self.dW1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        self.W2 -= self.learning_rate * self.dW2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \n",
    "        Args:\n",
    "            X: Training data\n",
    "            y: Training labels (one-hot encoded)\n",
    "            epochs: Number of training epochs\n",
    "            verbose: Whether to print training progress\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            predictions = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, predictions)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            accuracy = self.compute_accuracy(y, predictions)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Backward propagation\n",
    "            self.backward_propagation(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters()\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Predicted class probabilities\n",
    "            predicted_classes: Predicted class labels\n",
    "        \"\"\"\n",
    "        predictions = self.forward_propagation(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        return predictions, predicted_classes\n",
    "    \n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (one-hot encoded)\n",
    "            y_pred: Predicted probabilities\n",
    "            \n",
    "        Returns:\n",
    "            accuracy: Classification accuracy\n",
    "        \"\"\"\n",
    "        predicted_classes = np.argmax(y_pred, axis=1)\n",
    "        true_classes = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "        return accuracy\n",
    "\n",
    "print(\"üèóÔ∏è Neural Network class created successfully!\")\n",
    "print(\"Ready to build and train our model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "# 5. Training Our Neural Network üöÄ\n",
    "\n",
    "Now let's create and train our neural network on the Iris dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-and-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network\n",
    "# Architecture: 4 inputs ‚Üí 8 hidden neurons ‚Üí 3 outputs\n",
    "nn = NeuralNetwork(input_size=4, hidden_size=8, output_size=3, learning_rate=0.1)\n",
    "\n",
    "print(\"üß† Neural Network Architecture:\")\n",
    "print(f\"Input Layer: {nn.input_size} neurons\")\n",
    "print(f\"Hidden Layer: {nn.hidden_size} neurons (Sigmoid activation)\")\n",
    "print(f\"Output Layer: {nn.output_size} neurons (Softmax activation)\")\n",
    "print(f\"Learning Rate: {nn.learning_rate}\")\n",
    "print(\"\\nTraining the neural network...\\n\")\n",
    "\n",
    "# Train the network\n",
    "nn.train(X_train_scaled, y_train_onehot, epochs=1000, verbose=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation and Visualization üìä\n",
    "\n",
    "Let's evaluate our trained neural network and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both training and test sets\n",
    "train_predictions, train_pred_classes = nn.predict(X_train_scaled)\n",
    "test_predictions, test_pred_classes = nn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, train_pred_classes)\n",
    "test_accuracy = accuracy_score(y_test, test_pred_classes)\n",
    "\n",
    "print(\"üéØ Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìà Detailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, test_pred_classes, target_names=iris.target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, test_pred_classes)\n",
    "print(\"\\nüîç Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(nn.loss_history, 'b-', linewidth=2)\n",
    "axes[0].set_title('Training Loss Over Time')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-Entropy Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(nn.accuracy_history, 'g-', linewidth=2)\n",
    "axes[1].set_title('Training Accuracy Over Time')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìâ Final Training Loss: {nn.loss_history[-1]:.4f}\")\n",
    "print(f\"üìà Final Training Accuracy: {nn.accuracy_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some prediction examples\n",
    "print(\"üîÆ Prediction Examples (Test Set):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(10):  # Show first 10 test examples\n",
    "    true_class = y_test[i]\n",
    "    pred_class = test_pred_classes[i]\n",
    "    confidence = np.max(test_predictions[i])\n",
    "    \n",
    "    true_name = iris.target_names[true_class]\n",
    "    pred_name = iris.target_names[pred_class]\n",
    "    \n",
    "    status = \"‚úÖ\" if true_class == pred_class else \"‚ùå\"\n",
    "    \n",
    "    print(f\"{status} Sample {i+1}: True: {true_name:<12} | Predicted: {pred_name:<12} | Confidence: {confidence:.3f}\")\n",
    "\n",
    "print(\"\\nüé≤ Prediction Probabilities for First 5 Test Samples:\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: {test_predictions[i]} (True: {iris.target_names[y_test[i]]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-section",
   "metadata": {},
   "source": [
    "# 7. Exercises and Challenges üí™\n",
    "\n",
    "Now it's your turn to experiment and learn! Try these exercises to deepen your understanding:\n",
    "\n",
    "## üéØ Exercise 1: Experiment with Different Architectures\n",
    "Try changing the number of hidden neurons and see how it affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different hidden layer sizes\n",
    "# Try: 4, 8, 16, 32 hidden neurons\n",
    "\n",
    "hidden_sizes = [4, 8, 16, 32]\n",
    "results = {}\n",
    "\n",
    "print(\"üß™ Experimenting with different hidden layer sizes...\\n\")\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"Testing with {hidden_size} hidden neurons...\")\n",
    "    \n",
    "    # Create and train network\n",
    "    nn_exp = NeuralNetwork(input_size=4, hidden_size=hidden_size, \n",
    "                          output_size=3, learning_rate=0.1)\n",
    "    nn_exp.train(X_train_scaled, y_train_onehot, epochs=1000, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    _, test_pred = nn_exp.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    results[hidden_size] = test_acc\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values(), color='skyblue', alpha=0.8)\n",
    "plt.title('Test Accuracy vs Hidden Layer Size')\n",
    "plt.xlabel('Number of Hidden Neurons')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "for size, acc in results.items():\n",
    "    plt.text(size, acc + 0.01, f'{acc:.3f}', ha='center')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° What do you notice about the relationship between hidden layer size and performance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-2",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Learning Rate Sensitivity\n",
    "Investigate how different learning rates affect training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "lr_results = {}\n",
    "\n",
    "print(\"üéõÔ∏è Experimenting with different learning rates...\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"Testing with learning rate = {lr}...\")\n",
    "    \n",
    "    # Create and train network\n",
    "    nn_lr = NeuralNetwork(input_size=4, hidden_size=8, \n",
    "                         output_size=3, learning_rate=lr)\n",
    "    nn_lr.train(X_train_scaled, y_train_onehot, epochs=500, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    _, test_pred = nn_lr.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    lr_results[lr] = test_acc\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(nn_lr.loss_history, label=f'LR={lr}')\n",
    "    plt.title(f'Learning Rate: {lr}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if i == len(learning_rates) - 1:  # Last subplot - show comparison\n",
    "        plt.subplot(2, 3, i+2)\n",
    "        plt.bar(lr_results.keys(), lr_results.values(), color='lightcoral', alpha=0.8)\n",
    "        plt.title('Final Test Accuracy')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Accuracy')\n",
    "        for lr_val, acc in lr_results.items():\n",
    "            plt.text(lr_val, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Questions to consider:\")\n",
    "print(\"- Which learning rate converged fastest?\")\n",
    "print(\"- Which learning rate achieved the best final accuracy?\")\n",
    "print(\"- What happens with very high learning rates?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-3",
   "metadata": {},
   "source": [
    "## üéØ Exercise 3: Different Activation Functions\n",
    "Modify the neural network to use different activation functions in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a modified neural network class that can use different activation functions\n",
    "# Hint: You'll need to modify the forward_propagation and backward_propagation methods\n",
    "\n",
    "class FlexibleNeuralNetwork(NeuralNetwork):\n",
    "    \"\"\"Neural network with configurable activation function\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1, activation='sigmoid'):\n",
    "        super().__init__(input_size, hidden_size, output_size, learning_rate)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation with configurable activation function\"\"\"\n",
    "        # Hidden layer\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        # Apply chosen activation function\n",
    "        if self.activation == 'sigmoid':\n",
    "            self.A1 = ActivationFunctions.sigmoid(self.Z1)\n",
    "        elif self.activation == 'relu':\n",
    "            self.A1 = ActivationFunctions.relu(self.Z1)\n",
    "        elif self.activation == 'tanh':\n",
    "            self.A1 = ActivationFunctions.tanh(self.Z1)\n",
    "        \n",
    "        # Output layer (always softmax for classification)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = ActivationFunctions.softmax(self.Z2)\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def backward_propagation(self, X, y_true):\n",
    "        \"\"\"Backpropagation with configurable activation function\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - y_true\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        \n",
    "        # Apply appropriate derivative\n",
    "        if self.activation == 'sigmoid':\n",
    "            dZ1 = dA1 * ActivationFunctions.sigmoid_derivative(self.A1)\n",
    "        elif self.activation == 'relu':\n",
    "            dZ1 = dA1 * ActivationFunctions.relu_derivative(self.Z1)\n",
    "        elif self.activation == 'tanh':\n",
    "            dZ1 = dA1 * ActivationFunctions.tanh_derivative(self.A1)\n",
    "        \n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dW1, self.db1 = dW1, db1\n",
    "        self.dW2, self.db2 = dW2, db2\n",
    "\n",
    "# Test different activation functions\n",
    "activations = ['sigmoid', 'relu', 'tanh']\n",
    "activation_results = {}\n",
    "\n",
    "print(\"üîß Testing different activation functions...\\n\")\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"Testing with {activation} activation...\")\n",
    "    \n",
    "    # Create and train network\n",
    "    nn_act = FlexibleNeuralNetwork(input_size=4, hidden_size=8, \n",
    "                                  output_size=3, learning_rate=0.1, \n",
    "                                  activation=activation)\n",
    "    nn_act.train(X_train_scaled, y_train_onehot, epochs=1000, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    _, test_pred = nn_act.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    activation_results[activation] = {\n",
    "        'accuracy': test_acc,\n",
    "        'loss_history': nn_act.loss_history\n",
    "    }\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curves\n",
    "for activation in activations:\n",
    "    axes[0].plot(activation_results[activation]['loss_history'], \n",
    "                label=f'{activation.capitalize()}', linewidth=2)\n",
    "axes[0].set_title('Training Loss by Activation Function')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Final accuracies\n",
    "accuracies = [activation_results[act]['accuracy'] for act in activations]\n",
    "axes[1].bar(activations, accuracies, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "axes[1].set_title('Final Test Accuracy by Activation Function')\n",
    "axes[1].set_xlabel('Activation Function')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_ylim(0.8, 1.0)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    axes[1].text(i, acc + 0.01, f'{acc:.3f}', ha='center')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Observations:\")\n",
    "for activation in activations:\n",
    "    acc = activation_results[activation]['accuracy']\n",
    "    print(f\"- {activation.capitalize()}: {acc:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge-section",
   "metadata": {},
   "source": [
    "## üèÜ Challenge: Build a Deeper Network\n",
    "\n",
    "Can you modify the neural network to have multiple hidden layers? This is more advanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deep-network-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deep neural network with multiple hidden layers\n",
    "# This is a challenging exercise - try to implement a network with 2-3 hidden layers\n",
    "\n",
    "print(\"üèÜ CHALLENGE: Build a Deep Neural Network\")\n",
    "print(\"=\"*50)\n",
    "print(\"Task: Implement a neural network with multiple hidden layers\")\n",
    "print(\"Suggested architecture: 4 ‚Üí 16 ‚Üí 8 ‚Üí 4 ‚Üí 3\")\n",
    "print(\"\")\n",
    "print(\"Tips:\")\n",
    "print(\"- You'll need to track multiple weight matrices and bias vectors\")\n",
    "print(\"- Forward propagation becomes more complex with more layers\")\n",
    "print(\"- Backpropagation requires careful chain rule application\")\n",
    "print(\"- Consider using ReLU activation for hidden layers to avoid vanishing gradients\")\n",
    "print(\"\")\n",
    "print(\"This is an advanced exercise - don't worry if it's challenging!\")\n",
    "print(\"The key is understanding the concepts we've learned so far.\")\n",
    "\n",
    "# Starter code structure (you fill in the implementation)\n",
    "class DeepNeuralNetwork:\n",
    "    \"\"\"Deep neural network with multiple hidden layers\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize deep neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        # TODO: Initialize weights and biases for multiple layers\n",
    "        pass\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation through multiple layers\"\"\"\n",
    "        # TODO: Implement forward pass through all layers\n",
    "        pass\n",
    "    \n",
    "    def backward_propagation(self, X, y_true):\n",
    "        \"\"\"Backpropagation through multiple layers\"\"\"\n",
    "        # TODO: Implement backward pass through all layers\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        \"\"\"Train the deep network\"\"\"\n",
    "        # TODO: Implement training loop\n",
    "        pass\n",
    "\n",
    "print(\"\\nü§î Think about:\")\n",
    "print(\"- How would you store weights for multiple layers?\")\n",
    "print(\"- How does backpropagation change with more layers?\")\n",
    "print(\"- What are the advantages and disadvantages of deeper networks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "# 8. Next Steps and Advanced Topics üöÄ\n",
    "\n",
    "Congratulations! You've successfully built and trained a neural network from scratch. Here's what you've learned:\n",
    "\n",
    "## ‚úÖ What You've Accomplished:\n",
    "- Implemented forward propagation\n",
    "- Implemented backpropagation and gradient descent\n",
    "- Trained a neural network on real data\n",
    "- Evaluated model performance\n",
    "- Experimented with different hyperparameters\n",
    "\n",
    "## üéØ Advanced Topics to Explore Next:\n",
    "\n",
    "### 1. **Regularization Techniques**\n",
    "- L1/L2 regularization to prevent overfitting\n",
    "- Dropout for better generalization\n",
    "- Early stopping\n",
    "\n",
    "### 2. **Optimization Algorithms**\n",
    "- Momentum\n",
    "- Adam optimizer\n",
    "- Learning rate scheduling\n",
    "\n",
    "### 3. **Deep Learning Frameworks**\n",
    "- TensorFlow/Keras\n",
    "- PyTorch\n",
    "- JAX\n",
    "\n",
    "### 4. **Advanced Architectures**\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Transformer networks\n",
    "\n",
    "### 5. **Specialized Applications**\n",
    "- Computer Vision\n",
    "- Natural Language Processing\n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and resources\n",
    "print(\"üéâ CONGRATULATIONS! üéâ\")\n",
    "print(\"=\"*50)\n",
    "print(\"You have successfully completed the Deep Learning from Scratch tutorial!\")\n",
    "print(\"\")\n",
    "print(\"üìä Your Neural Network Performance Summary:\")\n",
    "print(f\"- Final Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"- Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"- Network Architecture: {nn.input_size} ‚Üí {nn.hidden_size} ‚Üí {nn.output_size}\")\n",
    "print(f\"- Training Epochs: 1000\")\n",
    "print(f\"- Final Loss: {nn.loss_history[-1]:.4f}\")\n",
    "print(\"\")\n",
    "print(\"üß† Key Concepts Mastered:\")\n",
    "concepts = [\n",
    "    \"Neural network architecture\",\n",
    "    \"Forward propagation\",\n",
    "    \"Backpropagation algorithm\",\n",
    "    \"Gradient descent optimization\",\n",
    "    \"Activation functions\",\n",
    "    \"Loss functions (cross-entropy)\",\n",
    "    \"Model evaluation metrics\",\n",
    "    \"Hyperparameter tuning\"\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(concepts, 1):\n",
    "    print(f\"  {i}. ‚úÖ {concept}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"üìö Recommended Next Steps:\")\n",
    "print(\"1. Implement regularization techniques (L2, Dropout)\")\n",
    "print(\"2. Try the network on different datasets (Boston Housing, Wine, etc.)\")\n",
    "print(\"3. Experiment with deeper architectures\")\n",
    "print(\"4. Learn about convolutional neural networks for image data\")\n",
    "print(\"5. Explore modern deep learning frameworks (PyTorch, TensorFlow)\")\n",
    "print(\"\")\n",
    "print(\"üåü Keep learning and building amazing AI applications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "# üìñ Additional Resources\n",
    "\n",
    "## Books:\n",
    "- **\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville** - The comprehensive mathematical foundation\n",
    "- **\"Neural Networks and Deep Learning\" by Michael Nielsen** - Excellent intuitive explanations\n",
    "- **\"Hands-On Machine Learning\" by Aur√©lien G√©ron** - Practical implementation focus\n",
    "\n",
    "## Online Courses:\n",
    "- **Andrew Ng's Deep Learning Specialization (Coursera)** - Systematic and thorough\n",
    "- **Fast.ai Practical Deep Learning** - Top-down approach\n",
    "- **CS231n (Stanford)** - Computer vision focus\n",
    "\n",
    "## Practice Platforms:\n",
    "- **Kaggle** - Real-world datasets and competitions\n",
    "- **Google Colab** - Free GPU access for experimentation\n",
    "- **Papers With Code** - Latest research with implementations\n",
    "\n",
    "## Key Mathematical Topics to Strengthen:\n",
    "- Linear Algebra (matrices, eigenvalues, SVD)\n",
    "- Calculus (partial derivatives, chain rule)\n",
    "- Probability and Statistics\n",
    "- Information Theory\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The best way to learn deep learning is by doing. Keep experimenting, building projects, and don't be afraid to make mistakes. Every expert was once a beginner! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
